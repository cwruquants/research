{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9626566e",
   "metadata": {},
   "source": [
    "__Testing of the Attribute Classes and the Storage of Values for Sentiment Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d4ed64",
   "metadata": {},
   "source": [
    "1. Import extract text functions\n",
    "2. Execute these functions on the filepath of the earnings call xml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf696ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decompose_transcript import extract_presentation_section, extract_qa_section, clean_spoken_content\n",
    "qa_section = clean_spoken_content(extract_qa_section(\"../../data/earnings_calls/ex1.xml\"))\n",
    "presentation_section = clean_spoken_content(extract_presentation_section(\"../../data/earnings_calls/ex1.xml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd50d0f",
   "metadata": {},
   "source": [
    "1. Import DocumentAttr class\n",
    "2. Pass in the string, and which stored values that you wish to observe, note that the classes are inherited from largest to smallest text block (i.e ou can't store values for individual words without having stored values for sentences and paragraphs, or sentences without storing values for paragraphs, etc...)\n",
    "\n",
    "Here I initialize the values for documents ,paragraphs, sentences, and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dbe3573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kstry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from attribute import DocumentAttr\n",
    "qa_section = DocumentAttr(qa_section, store_paragraphs=True, store_sentences=True, store_words=True)\n",
    "presentation_section = DocumentAttr(presentation_section, store_paragraphs=True, store_sentences=True, store_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf953623",
   "metadata": {},
   "source": [
    "Initialize the Setup class with replaced file path and sheet names for ML words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "612c018e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kstry\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from setup import Setup\n",
    "setup = Setup(\n",
    "    sheet_name_positive='ML_negative_unigram',\n",
    "    sheet_name_negative='ML_positive_unigram',\n",
    "    file_path=r\"C:\\Users\\kstry\\Downloads\\Garcia_MLWords.xlsx\",\n",
    "    hf_model='cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "    device=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6099b6dd",
   "metadata": {},
   "source": [
    "Here is an example of using the fit_all recursion method which recursively calls the fit method through all the attribute classes, from document down to words. I print the document level attributes and export the values to an output json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9d638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Sentiment: 0.1794646005630493\n",
      "Document LM: -44.0\n",
      "Document ML: -10.0\n",
      "Document HIV4: 139.0\n"
     ]
    }
   ],
   "source": [
    "presentation_section = setup.fit_all(presentation_section)\n",
    "print(\"Document Sentiment:\", presentation_section.sentiment)\n",
    "print(\"Document LM:\", presentation_section.LM)\n",
    "print(\"Document ML:\", presentation_section.ML)\n",
    "print(\"Document HIV4:\", presentation_section.HIV4)\n",
    "setup.export_to_json(presentation_section, \"output.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
