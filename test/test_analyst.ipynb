{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook for Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/efang/Desktop/coding/research\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from src.analysis.analyst_module import Analyst\n",
    "from src.document.abstract_classes.setup_module import Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Setup configuration\n",
    "setup = Setup(\n",
    "    sheet_name_positive='ML_positive_unigram', # name of positive page in the Garcia_MLWords excel file\n",
    "    sheet_name_negative='ML_negative_unigram', # name of negative page in the Garcia_MLWords excel file \n",
    "    file_path=\"data/word_sets/Garcia_MLWords.xlsx\", # relative path\n",
    "    hf_model='cardiffnlp/twitter-roberta-base-sentiment-latest', # name of huggingface model\n",
    "    device=-1 # device to be using the model on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed!\n",
      "Output directory: results/Ryder_System_Inc_Q4_2015_20250910_215845\n",
      "TOML file: results/Ryder_System_Inc_Q4_2015_20250910_215845/analysis_metadata.toml\n",
      "Exposure results: results/Ryder_System_Inc_Q4_2015_20250910_215845/exposure_results.json\n",
      "\n",
      "Sentiment Analysis Summary:\n",
      "Q&A Section - Sentiment: 0.1795, ML: 10.0, LM: -44.0, HIV4: 139.0\n",
      "Presentation Section - Sentiment: 0.1209, ML: 11.0, LM: -35.0, HIV4: 106.0\n",
      "\n",
      "Matching Analysis Summary:\n",
      "Total keywords searched: 57\n",
      "Total matches found: 16\n",
      "Keywords with matches: 7\n"
     ]
    }
   ],
   "source": [
    "# Create an analyst with keyword path\n",
    "analyst = Analyst(keyword_path=\"data/paper_word_sets/political_words.csv\")\n",
    "\n",
    "# Test with a sample earnings call\n",
    "earnings_call_path = \"data/earnings_calls/ex1.xml\"\n",
    "\n",
    "# Run the analysis - this will create a new directory in results/\n",
    "result = analyst.fit_single_document(\n",
    "    earnings_call_path=earnings_call_path,\n",
    "    similarity=\"cosine\",\n",
    ")\n",
    "\n",
    "print(\"Analysis completed!\")\n",
    "print(f\"Output directory: {result['output_directory']}\")\n",
    "print(f\"TOML file: {result['toml_path']}\")\n",
    "print(f\"Exposure results: {result['exposure_results_path']}\")\n",
    "\n",
    "# Show some summary statistics\n",
    "qa_fit = result['qa_fit']\n",
    "pres_fit = result['pres_fit']\n",
    "exposure_results = result['exposure_results']\n",
    "\n",
    "print(f\"\\nSentiment Analysis Summary:\")\n",
    "print(f\"Q&A Section - Sentiment: {qa_fit.sentiment:.4f}, ML: {qa_fit.ML}, LM: {qa_fit.LM}, HIV4: {qa_fit.HIV4}\")\n",
    "print(f\"Presentation Section - Sentiment: {pres_fit.sentiment:.4f}, ML: {pres_fit.ML}, LM: {pres_fit.LM}, HIV4: {pres_fit.HIV4}\")\n",
    "\n",
    "print(f\"\\nMatching Analysis Summary:\")\n",
    "print(f\"Total keywords searched: {exposure_results.total_keywords_searched}\")\n",
    "print(f\"Total matches found: {exposure_results.total_direct_matches + exposure_results.total_cosine_matches}\")\n",
    "print(f\"Keywords with matches: {exposure_results.total_keywords_with_matches}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
